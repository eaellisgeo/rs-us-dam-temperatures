{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### This Notebook is for Calculating Accuracy Assessment #######\n",
    "## First, pull all available USGS temperature gage data. Final results only use gages that occur within the same Landsat scenes as another gage ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages needed\n",
    "import numpy as np\n",
    "from dataretrieval import nwis\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "import os\n",
    "import glob\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "import datetime as dt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Prepare the Data ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dam AOI File\n",
    "DamAOIs = gpd.read_file(r\"F:\\Insert_File_Path_of_Shapefile_with_Dam_AOIs.shp\") # Update this file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select USGS Site Numbers for the Study ###\n",
    "# Set Parameters\n",
    "P_Code = '00010'\n",
    "StartDate = '2013-03-01'\n",
    "EndDate = '2023-12-31'\n",
    "\n",
    "## Pull in List of Gages -- List was pulled from USGS Website\n",
    "Unique_Gages =  pd.read_excel(r\"F:\\Insert_File_Path_of_All_Possible_Gage_Site_Nos.xlsx\", sheet_name='Sheet1', converters={'site_no':str}) #converter keeps padded zero # Update this file path here\n",
    "# Drop Null in Location Values\n",
    "Unique_Gages = Unique_Gages[(Unique_Gages['dec_long_va'].notna())& (Unique_Gages['dec_lat_va'].notna())]\n",
    "Unique_Gages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need distances later to make sure these nodes are near our larger rivers ### \n",
    "# Convert to GDF \n",
    "Unique_Gages_gdf = gpd.GeoDataFrame(Unique_Gages, geometry=gpd.points_from_xy(x=Unique_Gages.dec_long_va, y=Unique_Gages.dec_lat_va))\n",
    "\n",
    "# Pull in SWORD Nodes\n",
    "SWORD_Nodes = gpd.read_file(r\"F:\\Insert_File_Path_of_the_Shapefile_Containing_the_Selected_SWORD_Nodes.shp\") # Update this file path\n",
    "# Rename Width Column\n",
    "SWORD_Nodes.rename(columns={\"width\":\"SWD_wid\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Nearest Node Function For Later ##\n",
    "def Closest_Nodes(gdA, gdB):\n",
    "\n",
    "    nA = np.array(list(gdA.geometry.apply(lambda x: (x.x, x.y))))\n",
    "    nB = np.array(list(gdB.geometry.apply(lambda x: (x.x, x.y))))\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "    gdB_nearest = gdB.iloc[idx].drop(columns=\"geometry\").reset_index(drop=True)\n",
    "    gdf = pd.concat(\n",
    "        [\n",
    "            gdA.reset_index(drop=True),\n",
    "            gdB_nearest,\n",
    "            pd.Series(dist, name='dist')\n",
    "        ], \n",
    "        axis=1)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CRS (My Data is WGS 84, USGS is NAD 83), Match Them, Find points within AOIs\n",
    "Unique_Gages_gdf_crs = Unique_Gages_gdf.set_crs('EPSG:4269')\n",
    "Unique_Gages_gdf_crs = Unique_Gages_gdf_crs.to_crs('EPSG:4326')\n",
    "NN_Gage_Filt = gpd.sjoin(Unique_Gages_gdf_crs, DamAOIs, predicate = 'within')\n",
    "NN_Gage_Filt = NN_Gage_Filt.drop_duplicates(subset='site_no', keep='last')\n",
    "\n",
    "# Save File and Preview\n",
    "NN_Gage_Filt.to_csv(r\"F:\\Insert_File_Path_for_List_USGS_Gages_SWORD.csv\")  # Update this file path\n",
    "print(\"Gages within AOIs: \" + str(len(NN_Gage_Filt)))\n",
    "NN_Gage_Filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pull USGS Data ###\n",
    "# Get a list of Site Numbers\n",
    "Selected_Sites_List = NN_Gage_Filt['site_no'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pull Gage Data from USGS and Save it #####\n",
    "### Takes a long time -- Running only once is recommended ####\n",
    "\n",
    "# Set Parameters for loop\n",
    "Outputpath = r\"F:\\Insert_File_Path_for_Saving_Gage_Data\"  # Update this file path\n",
    "\n",
    "# This is done to have the data saved and also to improve run times (Big Datasets)\n",
    "for i in Selected_Sites_List:\n",
    "  try:\n",
    "    TempGage = nwis.get_record(sites =  i , service = 'iv', parameterCd =  P_Code, start = StartDate, end = EndDate)\n",
    "    TempGage.rename(columns = {'00010':'Temperature'}, inplace = True)\n",
    "    # Save the CSV\n",
    "    TempGage.to_csv(Outputpath +  r\"\\Gage_\"+ str(i))\n",
    "    print(\"Gage \" + str(i) + \" CSV Exported\")\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the List of Gage Data\n",
    "Outputpath =  r\"F:\\Insert_File_Path_of_Folder_of_Saved_Gage_Data\"  # Update this file path\n",
    "Gage_Files = glob.glob(os.path.join(Outputpath, \"Gage_*\")) \n",
    "SplitString = \"Gage_\"\n",
    "# Create a list for non-empty files\n",
    "Gage_List = []\n",
    "SplitString = \"Gage_\" # will use this to pull the Gage number\n",
    "\n",
    "# Loop thru each gage file and check if it is empty, some gages don't have data\n",
    "for i in Gage_Files: \n",
    "    # If the file is greater than 1KB --> list\n",
    "    check_file = os.path.getsize(i) # note this is done in bytes (1KB = 1000bytes)\n",
    "    # If it has data -- add to list \n",
    "    if(check_file > 1000):\n",
    "        gage_no =i.split(SplitString)[1]\n",
    "        Gage_List.append(gage_no)\n",
    "\n",
    "    # If is is empty -- skip\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(\"Number of Gages Exported: \"+ str(len(Gage_Files)))\n",
    "print( \"Total Gages with Data: \" + str(len(Gage_List)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For reference: these missing gages are because of the study period mismatch ##\n",
    "EmptyGages = NN_Gage_Filt[(~NN_Gage_Filt['site_no'].isin(Gage_List))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Get which Dam's RWCT to pull in -- Data is stored by dam \n",
    "# Get a list of Dams to pull in RWCT Values\n",
    "RWCT_AA_List = NN_Gage_Filt['Assgn_dam'].drop_duplicates().tolist()\n",
    "RWCT_AA_List.sort()\n",
    "print(\"Number of Assiocated Dams:  \" + str(len(RWCT_AA_List)))\n",
    "print(RWCT_AA_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Look at the 5 Closest Temperature Nodes ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to pull the accuracy like the analysis does (temperature is the average of 5 closest nodes)\n",
    "GageTemps_5n = pd.DataFrame()\n",
    "\n",
    "for i in RWCT_AA_List: \n",
    "    try:\n",
    "        # Pull in Temp Points based on the dam\n",
    "        RWCT_Data = pd.read_csv(r\"Insert_File_Path_of_the_Folder_of_Temperature_Outputs_from_RWC\\Dam_\"+ str(int(i)) +r\"RWCT.csv\" ,\n",
    "                                usecols = ['Clst_Dam', 'GEE_temp', 'flag_cldShadow', 'flag_cloud', 'flag_water', 'image_id', 'latitude', 'longitude', 'RWC_wid', 'Date_Time', 'Date', 'Month', 'Year', 'Season'], # Keep only the columns needed for accuracy assessment\n",
    "                                engine='python')\n",
    "        geometry = [Point(xy) for xy in zip(RWCT_Data['longitude'], RWCT_Data['latitude'])]\n",
    "        RWCT_Data_gdf = gpd.GeoDataFrame(RWCT_Data, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Get the gages assoc with that dam\n",
    "        Dam_Gages = NN_Gage_Filt[NN_Gage_Filt['Assgn_dam']== i]\n",
    "        Dam_Gages = Dam_Gages[['agency_cd', 'site_no', 'station_nm', 'site_tp_cd', 'dec_lat_va',  'dec_long_va', 'huc_cd']] # Keep only the columns you need for accuracy assessment\n",
    "        Dam_Gages = Dam_Gages[(Dam_Gages['site_no'].isin(Gage_List))] # Filter only the Sites with data\n",
    "        geometry2 = [Point(xy) for xy in zip(Dam_Gages[ 'dec_long_va'], Dam_Gages[ 'dec_lat_va'])]\n",
    "        Dam_Gages_gdf = gpd.GeoDataFrame(Dam_Gages, geometry=geometry2, crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Get the unique image date/times\n",
    "        RWCT_Times = RWCT_Data_gdf['Date_Time'].drop_duplicates().tolist()\n",
    "        for j in RWCT_Times:\n",
    "            OneImage = RWCT_Data_gdf[RWCT_Data_gdf['Date_Time']== j]\n",
    "            Image_Dist = Closest_Nodes(OneImage, Dam_Gages_gdf)\n",
    "            # Nearest Distance is calculated in Degrees b/c both gdf are in WGS 84 --> (Convert Degrees to meters * 111139)\n",
    "            Image_Dist['NDist_m'] = Image_Dist['dist']*111139 \n",
    "\n",
    "            ## Filtering to the closest nodes for averaging temps ##\n",
    "            # Get each Gage's 5 nearest RWC points for each date\n",
    "            Nearest_RWCT_grp = Image_Dist.groupby(['site_no','Date_Time'])['dist'].nsmallest(5)\n",
    "            Nearest_RWCT_grp = Nearest_RWCT_grp.reset_index()\n",
    "\n",
    "            # Get a list of Index numbers\n",
    "            Node_list5 = Nearest_RWCT_grp['level_2'].tolist()\n",
    "\n",
    "            # Select nodes by Index\n",
    "            Select_ImageNodes = Image_Dist.iloc[Node_list5]\n",
    "            \n",
    "            # Remove extra nodes -- If it is farther than 200m drop it\n",
    "            Image_Dist_filt = Select_ImageNodes[Select_ImageNodes['NDist_m'] <= 200]\n",
    "            \n",
    "            #### Get the Temperature Averages ####\n",
    "            ## Group By Node, Month, and Year -- Get Average Temp \n",
    "            Avg_Near_Temps = Image_Dist_filt.groupby(['site_no','Date_Time']).agg({'GEE_temp': ['mean'],'RWC_wid': ['mean']})\n",
    "            Avg_Near_Temps.columns = ['Avg_Temp', 'Avg_RWC_Wid']\n",
    "            Avg_Near_Temps = Avg_Near_Temps.reset_index()\n",
    "\n",
    "            # Convert to Dataframe\n",
    "            AvgTemps_df = pd.DataFrame(Avg_Near_Temps)\n",
    "        \n",
    "            # Join The information \n",
    "            AvgMonTemps_xy = pd.merge(AvgTemps_df, Dam_Gages_gdf, on='site_no', how='inner')\n",
    "\n",
    "            GageTemps_5n = pd.concat([GageTemps_5n,AvgMonTemps_xy],axis=0)\n",
    "            \n",
    "        print(\"Finished Dam: \"+ str(i))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "GageTemps_5n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "GageTemps_5n.to_csv(r\"F:\\Insert_File_Path_for\\RWCT_Gage_Temps_5n.csv\") # Update this file path\n",
    "print(\"Gage Temp 5n CSV Exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update some Fields for Assessment  ##\n",
    "# Make a copy of the RWCT temps at Gages\n",
    "GageTemps_5n_forjoin = pd.DataFrame(GageTemps_5n)\n",
    "\n",
    "# Round DateTime to Nearest Hour -- to pair up gage data\n",
    "GageTemps_5n_forjoin['Date_Time'] = pd.to_datetime(GageTemps_5n_forjoin['Date_Time'], format='mixed')\n",
    "GageTemps_5n_forjoin['Round_time'] = GageTemps_5n_forjoin['Date_Time'].dt.round('60min')\n",
    "\n",
    "print('Total Sites with Available RWCT Avg Data: '+ str(len(GageTemps_5n_forjoin['site_no'].drop_duplicates().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the gages and add temperatures\n",
    "Gage_RS_Temp_5n = pd.DataFrame()\n",
    "\n",
    "for i in Gage_List:\n",
    "    # Pull in and set up gage records\n",
    "    Gage_Record = pd.read_csv(r\"F:\\Insert_File_Path_of_Folder_of_Saved_Gage_Data\\Gage_\" + str(i), converters={'site_no': str}, engine = 'python')  # Update this file path\n",
    "    Gage_Record['Round_time'] =  pd.to_datetime(Gage_Record['datetime'])\n",
    "    Gage_Record['site_no'] = Gage_Record['site_no'].astype(str)\n",
    "\n",
    "     # Filter Site Info to the Site Number from table with GEE exports and nearest points\n",
    "    SIteInfo =  GageTemps_5n_forjoin[(GageTemps_5n_forjoin['site_no'] == i)]\n",
    "\n",
    "    # Merge the data together\n",
    "    MergedTemps_5n = pd.merge(SIteInfo, Gage_Record, on=['site_no', 'Round_time'], how='inner')\n",
    "\n",
    "    # Place in the Empty dataframe\n",
    "    Gage_RS_Temp_5n = pd.concat([Gage_RS_Temp_5n,MergedTemps_5n],axis=0)\n",
    "print(\"Combined: Measurements for all sites with available gage and satellite observations\")\n",
    "\n",
    "Gage_RS_Temp_5n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gage data has some messy naming conventions that need to be cleaned ###\n",
    "# Fix columns with spaces and punctuation in them\n",
    "Gage_RS_Temp_5n.columns = Gage_RS_Temp_5n.columns.str.replace(' ', '')\n",
    "Gage_RS_Temp_5n.columns = Gage_RS_Temp_5n.columns.str.replace(',', '')\n",
    "Gage_RS_Temp_5n.columns = Gage_RS_Temp_5n.columns.str.replace('[', '')\n",
    "Gage_RS_Temp_5n.columns = Gage_RS_Temp_5n.columns.str.replace(']', '')\n",
    "Gage_RS_Temp_5n.columns = Gage_RS_Temp_5n.columns.str.replace('/', '')\n",
    "\n",
    "# Save intermediate file -- all possible matching gage to landsat observations\n",
    "Gage_RS_Temp_5n.to_csv(r\"F:\\Insert_File_Path_For\\Matched_Gage_Temps_5n.csv\")  # Update this file path\n",
    "\n",
    "# Some columns have non-uniform naming conventions. We want to standardize them\n",
    "# Find the gages where \"Temperature\" is na\n",
    "NonTraditional_Temps = Gage_RS_Temp_5n[Gage_RS_Temp_5n[['Temperature']].isna().any(axis=1)]\n",
    "\n",
    "## Clean up columns to get one single temperature column ## \n",
    "# Column Types\n",
    "ysi_cols = [col for col in NonTraditional_Temps.columns if '_ysi' in col]\n",
    "up_cols = [col for col in NonTraditional_Temps.columns if '_up' in col]\n",
    "top_cols = [col for col in NonTraditional_Temps.columns if '_top' in col]\n",
    "left_cols = [col for col in NonTraditional_Temps.columns if '_left' in col]\n",
    "right_cols = [col for col in NonTraditional_Temps.columns if '_right' in col]\n",
    "cent_cols = [col for col in NonTraditional_Temps.columns if '_center' in col]\n",
    "flt_cols = [col for col in NonTraditional_Temps.columns if '_float' in col]\n",
    "p4_cols = [col for col in NonTraditional_Temps.columns if '_p4' in col]\n",
    "cs_cols = [col for col in NonTraditional_Temps.columns if '_cs' in col]\n",
    "therm_cols = [col for col in NonTraditional_Temps.columns if 'thermistor' in col]\n",
    "tdg_cols = [col for col in NonTraditional_Temps.columns if '_tdg' in col]\n",
    "# More Specific Column Types\n",
    "ds_cols = [col for col in NonTraditional_Temps.columns if '_downstreamhydro' in col]\n",
    "exo_cols = [col for col in NonTraditional_Temps.columns if '_exo1depthat3feet' in col]\n",
    "hrec_cols = [col for col in NonTraditional_Temps.columns if '_hrecos' in col]\n",
    "pier_cols = [col for col in NonTraditional_Temps.columns if 'pier' in col]\n",
    "\n",
    "# Combine them\n",
    "alttemp_cols = ysi_cols + up_cols + top_cols + left_cols + right_cols + cent_cols + flt_cols  + p4_cols + cs_cols + therm_cols + tdg_cols + ds_cols + exo_cols + hrec_cols + pier_cols\n",
    "\n",
    "# Remove the code columns\n",
    "suff = '_cd'\n",
    " \n",
    "# Suffix removal from String list\n",
    "# using loop + remove() + endswith()\n",
    "for word in alttemp_cols[:]:\n",
    "    if word.endswith(suff):\n",
    "        alttemp_cols.remove(word)\n",
    "\n",
    "print(alttemp_cols)\n",
    "\n",
    "# Get list of all the Temperature Columns\n",
    "USGS_Temp_Cols = alttemp_cols + ['00010_2'] + ['Temperature']\n",
    "\n",
    "# Get the Temps in one column -- Average because each row only has one temp value, rest are nulls\n",
    "Gage_RS_Temp_5n['USGS_Temp'] = Gage_RS_Temp_5n[USGS_Temp_Cols].mean(axis=1).astype(float)\n",
    "\n",
    "# Export the fixed matches to a CSV\n",
    "Gage_RS_Temp_5n.to_csv(r\"F:\\Insert_File_Path_for\\Matched_Gage_Temps_ColumnFix_5n.csv\")  # Update this file path\n",
    "print(\"Matched Gage Temp 5n -- Fixed Columns CSV Exported\")\n",
    "\n",
    "## For following analyses -- use a simplified DF\n",
    "QuickView_5n = Gage_RS_Temp_5n[['site_no','huc_cd','Date_Time','Avg_Temp', 'Avg_RWC_Wid', 'USGS_Temp']]\n",
    "\n",
    "# Remove Error Gage Values\n",
    "QuickView_5n = QuickView_5n[QuickView_5n['USGS_Temp']>= 0]\n",
    "QuickView_5n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Get Accuracy #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Error Metrics ##\n",
    "EPSILON = 1e-10\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return predicted - actual\n",
    "\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Squared Error \"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "\n",
    "def rmse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Root Mean Squared Error \"\"\"\n",
    "    return np.sqrt(mse(actual, predicted))\n",
    "\n",
    "def nrmse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Normalized Root Mean Squared Error \"\"\"\n",
    "    return rmse(actual, predicted) / (actual.max() - actual.min())\n",
    "\n",
    "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Absolute Error \"\"\"\n",
    "    return np.mean(np.abs(_error(actual, predicted)))\n",
    "\n",
    "def mbe(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Bias Error \"\"\"\n",
    "    return np.sum(_error(actual, predicted))/len(actual)\n",
    "\n",
    "METRICS = {\n",
    "    'rmse': rmse,\n",
    "    'nrmse': nrmse,\n",
    "    'mae': mae,\n",
    "    'mbe':mbe\n",
    "}\n",
    "\n",
    "def evaluate(actual: np.ndarray, predicted: np.ndarray, metrics=('rmse', 'nrmse', 'mae', 'mbe')):\n",
    "    results = {}\n",
    "    for name in metrics:\n",
    "        try:\n",
    "            results[name] = METRICS[name](actual, predicted)\n",
    "        except Exception as err:\n",
    "            results[name] = np.nan\n",
    "            print('Unable to compute metric {0}: {1}'.format(name, err))\n",
    "    return results\n",
    "\n",
    "def evaluate_all(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return evaluate(actual, predicted, metrics=set(METRICS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "### Check Wider & Error  Points ###\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check wider points ##\n",
    "# Get a copy of the data\n",
    "QuickView_5n = Gage_RS_Temp_5n[['site_no','huc_cd','Date_Time','Avg_Temp', 'Avg_RWC_Wid', 'USGS_Temp']] ## Note: we use datetime, becuase we need the timestamps to match images later\n",
    "\n",
    "# Remove error gage values\n",
    "QuickView_5n = QuickView_5n[QuickView_5n['USGS_Temp']>= 0]\n",
    "\n",
    "# Define the width bin edges\n",
    "bins = [QuickView_5n['Avg_RWC_Wid'].min(), 100,  300, 500, 1500, QuickView_5n['Avg_RWC_Wid'].max()]\n",
    "\n",
    "# Bin the width column\n",
    "QuickView_5n['Width_bin'] = pd.cut(QuickView_5n['Avg_RWC_Wid'], bins)\n",
    "\n",
    "# Remove observations < 100m in width\n",
    "QuickView_5n = QuickView_5n[ (QuickView_5n['Avg_RWC_Wid']>=100)]\n",
    "\n",
    "# Preview\n",
    "QuickView_5n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "################## Get RELATIVE ACCURACIES  ##################\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We want to compare relative accuracies -- Landsat delta vs Gage delta within a given image \n",
    "## So we need to figure out which of the above gages are within the same landsat images as each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a copy of the gages\n",
    "Filtered = QuickView_5n[:]\n",
    "\n",
    "## Get List of Gages -- Used to Compare the location to Landsat Scene footprints in Arc\n",
    "print(\"Number of Potential Sites: \" + str(len(Filtered['site_no'].drop_duplicates().to_list())))\n",
    "print(\"Site Numbers: \")\n",
    "print(Filtered['site_no'].drop_duplicates().to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GAGE PAIRS WERE DETERMINED MANUALLY IN ARCGIS ##########\n",
    "## Output is a csv file -- USGS Site numbers have leading zeros. Be cautious of opening the CSV file in Excel ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use all the possible pair options -- Created in ArcGIS and populated in a CSV file  ##  \n",
    "GagePairs = pd.read_csv(r\"F:\\Insert_File_Path_of_Matched_Gages\\Site_Image_Match.csv\", converters={'Site 1': str, 'Site 2': str }, engine = 'python') # Update this file path\n",
    "GagePairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join The gage and landsat Information with the corresponding sites\n",
    "AllSite_ImageOverlap = pd.DataFrame()\n",
    "\n",
    "for i in range(len(GagePairs)):\n",
    "    PairTest_1 = Filtered[Filtered['site_no']== GagePairs.iloc[i, 0]]\n",
    "    PairTest_2 = Filtered[Filtered['site_no']== GagePairs.iloc[i, 1]]\n",
    "\n",
    "    JoinSitePair = pd.merge(PairTest_1,PairTest_2, on= \"Date_Time\")\n",
    "    JoinSitePair = JoinSitePair[[\"site_no_x\", \"Date_Time\", \"Avg_Temp_x\",\"Avg_RWC_Wid_x\", \"USGS_Temp_x\",\"site_no_y\", \"Avg_Temp_y\", \"Avg_RWC_Wid_y\",\"USGS_Temp_y\"]]\n",
    "    JoinSitePair[\"Image_Diff\"] = JoinSitePair[\"Avg_Temp_x\"]-JoinSitePair[\"Avg_Temp_y\"]\n",
    "    JoinSitePair[\"Gage_Diff\"] = JoinSitePair[\"USGS_Temp_x\"]-JoinSitePair[\"USGS_Temp_y\"]\n",
    "\n",
    "    # Add to DF\n",
    "    output = pd.concat([AllSite_ImageOverlap, JoinSitePair],ignore_index=True)\n",
    "        \n",
    "    # Signif Output\n",
    "    AllSite_ImageOverlap = output\n",
    "\n",
    "AllSite_ImageOverlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Number of Usable Gages ##\n",
    "Site1List = AllSite_ImageOverlap[\"site_no_x\"].to_list()\n",
    "Site2List = AllSite_ImageOverlap[\"site_no_y\"].to_list()\n",
    "Crossover_Sites = (Site1List + Site2List)\n",
    "Crossover_Sites = list(set(Crossover_Sites))\n",
    "print(\"Number of gages with crossover: \", str(len(Crossover_Sites)))\n",
    "print(Crossover_Sites) # Used to make a final shapefile for study area map. Created in ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the temperature difference between image and gage measurement ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "## Calculate the Error Metrics\n",
    "ErrorMetrics_Cross = evaluate_all(actual= AllSite_ImageOverlap.Gage_Diff, predicted=AllSite_ImageOverlap.Image_Diff )\n",
    "print(ErrorMetrics_Cross)\n",
    "\n",
    "# Get the labels for the plots\n",
    "RMSE_Label =  str(round(ErrorMetrics_Cross.get('rmse'),1))\n",
    "MAE_Label = str(round(ErrorMetrics_Cross.get('mae'),1))\n",
    "Bias_Label = str(round(ErrorMetrics_Cross.get('mbe'),1))\n",
    "\n",
    "### Plot the Results ###\n",
    "\n",
    "# Get the data density for symbology/color ramp\n",
    "from scipy.stats import gaussian_kde\n",
    "xy = np.vstack([AllSite_ImageOverlap[\"Gage_Diff\"], AllSite_ImageOverlap[\"Image_Diff\"]])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Plot\n",
    "ax = sns.jointplot(data=AllSite_ImageOverlap, x=\"Gage_Diff\", y=\"Image_Diff\", xlim = (-10,12), ylim = (-10,12), space= 1,\n",
    "                  kind=\"kde\", fill = True, zorder = 1, alpha = 0)\n",
    "\n",
    "sns.scatterplot(data=AllSite_ImageOverlap, x=\"Gage_Diff\", y=\"Image_Diff\", marker = \"o\", edgecolor=None, s = 20, hue = z, palette = \"cividis\",  zorder = 2, legend = False)\n",
    "plt.plot([-10, 12], [-10, 12], zorder = 1,  color = 'grey', linestyle='dashed')\n",
    "\n",
    "# Label settings\n",
    "ax.set_axis_labels('Difference between gage measurements (°C)', 'Difference between TIR measurements (°C)', fontsize=12) \n",
    "ax.ax_joint.set_xticks([-10, -5, 0, 5, 10])\n",
    "ax.ax_joint.set_yticks([-10, -5, 0, 5, 10])\n",
    "\n",
    "# Color Bar\n",
    "norm = plt.Normalize(z.min(), z.max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"cividis\", norm=norm)\n",
    "sm.set_array([])\n",
    "plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)  # shrink figure  so cbar is visible\n",
    "# make new ax object for the cbar\n",
    "cbar_ax = ax.fig.add_axes([.85, .25, .05, .4])  # x, y, width, height\n",
    "plt.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "# Add the Labels to the figure \n",
    "plt.text(x= -12.5, y=0.11, s= \"RMSE: \"+ str(RMSE_Label) + \"°C\", fontsize=12, color='black')\n",
    "plt.text(x= -12.5, y=0.10, s= \"MAE: \"+ str(MAE_Label) + \"°C\", fontsize=12, color='black')\n",
    "plt.text(x= -12.5, y=0.09, s= \"Bias: \"+ str(Bias_Label) + \"°C\", fontsize=12, color='black')\n",
    "plt.text(x= -0.3 , y=0.113, s= \"Data \\nDensity\", fontsize=12, color='black')\n",
    "\n",
    "# Save the figure \n",
    "plt.savefig(r\"F:\\Insert_File_Path_for_Graphic_Output\\Accuracy_Assessment.png\", bbox_inches=\"tight\", pad_inches=0.25, dpi=1200)  # Update this file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Look At Accuracy Comparisons by Width ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a copy of the df\n",
    "Width_Accuracy = AllSite_ImageOverlap[:]\n",
    "\n",
    "# Get the minimum width between the two locations\n",
    "Width_Accuracy['Min_Width'] = Width_Accuracy[['Avg_RWC_Wid_x','Avg_RWC_Wid_y']].T.min()\n",
    "\n",
    "# Define the bin edges -- Min Width\n",
    "Min_bins = [100, 150, 200, 250, 300, 1500]\n",
    "\n",
    "# bin the Min Width  column\n",
    "Width_Accuracy['Min_Width_bin'] = pd.cut(Width_Accuracy['Min_Width'], Min_bins)\n",
    "\n",
    "# Export the widths -- will need for comparision graphic 2C\n",
    "Width_Accuracy.to_csv(r\"F:\\Insert_File_Path_For\\Widths_AA.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Accuracy by the Width Bins\n",
    "# Create Data Frame\n",
    "ErrorMetric_wid = pd.DataFrame()\n",
    "\n",
    "## Look at Accuracy by Site  -- keeping wider points only\n",
    "groups = Width_Accuracy.groupby('Min_Width_bin', observed= True)\n",
    "\n",
    "for name, group in groups:\n",
    "    try:\n",
    "        CompleteRows = group.loc[group.notna().all(axis='columns')]\n",
    "        # Create Linear Model\n",
    "        model = LinearRegression()\n",
    "        # Define predictor and response variables\n",
    "        X, y = CompleteRows[['Gage_Diff']], CompleteRows.Image_Diff\n",
    "\n",
    "        # Fit regression model\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Calculate R-squared\n",
    "        r_squared = model.score(X, y)\n",
    "\n",
    "        # Calculate Error Metrics\n",
    "        ErrorMetrics = evaluate_all(actual=CompleteRows.Gage_Diff, predicted=CompleteRows.Image_Diff)\n",
    "        # Convert Dictionary to DF\n",
    "        Error_Metric_df = pd.DataFrame(ErrorMetrics, index = [0])\n",
    "        # Attach Site No\n",
    "        Error_Metric_df['Min_Width_bin'] = name\n",
    "        \n",
    "        # Get Number of Points\n",
    "        Error_Metric_df['No_Points'] = len(CompleteRows)\n",
    "\n",
    "        # Concatenate \n",
    "        ErrorMetric_wid = pd.concat([ErrorMetric_wid, Error_Metric_df], axis = 0 )\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "ErrorMetric_wid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Width AA table\n",
    "ErrorMetric_wid.to_csv(r\"F:\\Insert_File_Path_For\\AA_by_widths.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
