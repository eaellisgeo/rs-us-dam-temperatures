{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## This Code is Used to filter UP/DS SWORD profiles to only include single dams ##########\n",
    "########## Snaps Temperatures to their nearest SWORD nodes and saves the CSV file  ##########\n",
    "########## Does not filter ANY GROD Dams (Except to run in loops) ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages needed\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "###### Bring in & Prep the Data #######\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Pull in the Dam File ###\n",
    "Dams = gpd.read_file(r\"F:\\Insert_File_Path_of_Shapefile_with_Dam_Locations.shp\") # Update this file path\n",
    "## This Shapefile ^^ has all the dams used to pull temperatures  in it with infromation from HILARRI matched to it (completed in ArcGIS) ##\n",
    "Dams_List  = Dams[\"grod_id\"].tolist()\n",
    "Dams_List.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the location of the RWC-Temp data\n",
    "TempsFilePath = r\"F:\\Insert_File_Path_of_Temperature_CSVs_from_GEE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bring in SWORD Nodes \n",
    "SWORD_Nodes = gpd.read_file(r\"Insert_File_Path_of_the_Shapefile_Containing_the_Selected_SWORD_Nodes.shp\") # Update this file path\n",
    "\n",
    "## Prep the Data ## \n",
    "# Rename Width Column\n",
    "SWORD_Nodes.rename(columns={\"width\":\"SWD_wid\"}, inplace = True)\n",
    "\n",
    "# Convert Dam Distances to km\n",
    "SWORD_Nodes['Dam_Dist_km'] = SWORD_Nodes['Dam_Dist']/1000\n",
    "\n",
    "## Add in Upstream & Downstream flags\n",
    "# Define Up/DS Function\n",
    "def get_upds(Dam_Distance):\n",
    "    if Dam_Distance == 0 :\n",
    "        return 'Dam'\n",
    "    if Dam_Distance > 0: \n",
    "        return 'Downstream'\n",
    "    else:\n",
    "        return 'Upstream'\n",
    "\n",
    "# Apply Up/DS Function\n",
    "SWORD_Nodes['Up_Ds'] = SWORD_Nodes.Dam_Dist.map(get_upds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subset SWORD -- Doing this manually \"parallelize\" the script\n",
    "# Create the List\n",
    "Dams_List_Group = Dams_List[:] # Change out these numbers for different groupings (0:410)\n",
    "\n",
    "# Subset the nodes\n",
    "Profile_Subset = SWORD_Nodes[SWORD_Nodes[\"Assgn_dam\"].isin(Dams_List_Group)]\n",
    "Profile_Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to remove anything downstream (DS) past another dam ##\n",
    "# Create a list to select by index numbers\n",
    "Updated_Profile_Indices = []\n",
    "\n",
    "# Loop through dams, find any additional DS dams, find the min distance, and filter nodes before dams\n",
    "for i in Dams_List_Group: \n",
    "    DS_Dams = Profile_Subset[(Profile_Subset[\"Assgn_dam\"] == i) & (Profile_Subset[\"Up_Ds\"] == \"Downstream\")& (Profile_Subset[\"Dam_Flag\"] == \"Dam\")]\n",
    "    if len(DS_Dams) > 0:\n",
    "        DS_Dam_Cutoff = DS_Dams[\"Dam_Dist_km\"].min() # Downstream nodes are positive, so closest is the smallest number\n",
    "        Nodes_Upd = Profile_Subset[(Profile_Subset[\"Assgn_dam\"] == i) & (Profile_Subset[\"Dam_Dist_km\"] < DS_Dam_Cutoff)] # Less than encompasses everything to the left of the cut off\n",
    "        Nodes_Upd_Index = Nodes_Upd.index.values.tolist()\n",
    "        Updated_Profile_Indices.extend(Nodes_Upd_Index)\n",
    "    else: # when there are no other dams ds, keep the original nodes\n",
    "        print(\"Dam \", str(i), \" has no downstream dams.\")\n",
    "        Keep_Nodes = Profile_Subset[(Profile_Subset[\"Assgn_dam\"] == i)]\n",
    "        Keep_Nodes_Index = Keep_Nodes.index.values.tolist()\n",
    "        Updated_Profile_Indices.extend(Keep_Nodes_Index)\n",
    "        print(\"Original Dam \", str(i), \" nodes included.\")\n",
    "\n",
    "# Use the index numbers selected in the for loop to filter the nodes\n",
    "Updated_Profiles = Profile_Subset.loc[ Profile_Subset.index.isin(Updated_Profile_Indices), : ]\n",
    "Updated_Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to remove anything upstream (UP) past another dam, using the updated profile from previous step ##\n",
    "# Create a list to select by index numbers\n",
    "Updated_Profile_Indices_UP = []\n",
    "\n",
    "# Loop through dams, find any additional UP dams, find the min distance, and filter nodes before dams\n",
    "for i in Dams_List_Group: \n",
    "    UP_Dams = Updated_Profiles[(Updated_Profiles[\"Assgn_dam\"] == i) & (Updated_Profiles[\"Up_Ds\"] == \"Upstream\")& (Updated_Profiles[\"Dam_Flag\"] == \"Dam\")]\n",
    "    if len(UP_Dams) > 0:\n",
    "        UP_Dam_Cutoff = UP_Dams[\"Dam_Dist_km\"].max() # Upstream nodes are negative, so closest is the largest number\n",
    "        Nodes_Upd = Updated_Profiles[(Updated_Profiles[\"Assgn_dam\"] == i) & (Updated_Profiles[\"Dam_Dist_km\"] > UP_Dam_Cutoff)] # Greater than encompasses everything to the right of the cut off\n",
    "        Nodes_Upd_Index = Nodes_Upd.index.values.tolist()\n",
    "        Updated_Profile_Indices_UP.extend(Nodes_Upd_Index)\n",
    "    else: # when there are no other dams up, keep the original nodes\n",
    "        print(\"Dam \", str(i), \" has no upstream dams.\")\n",
    "        Keep_Nodes = Updated_Profiles[(Updated_Profiles[\"Assgn_dam\"] == i)]\n",
    "        Keep_Nodes_Index = Keep_Nodes.index.values.tolist()\n",
    "        Updated_Profile_Indices_UP.extend(Keep_Nodes_Index)\n",
    "        print(\"Original Dam \", str(i), \" nodes included.\")\n",
    "\n",
    "# Use the index numbers selected in the for loop to filter the nodes\n",
    "Updated_Profiles_UPDS = Updated_Profiles.loc[ Updated_Profiles.index.isin(Updated_Profile_Indices_UP), : ]\n",
    "Updated_Profiles_UPDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define Functions ####\n",
    "#  Define Season Function\n",
    "def get_season(date):\n",
    "    year = str(date.year)\n",
    "    # Get the date string to use in the if statements\n",
    "    date_in = str(date)\n",
    "    # Format of the date_range (YYYY-MM-DD)\n",
    "    seasons = {'Spring': pd.date_range(start=year+'/03/01', end=year+'/05/31'),\n",
    "            'Summer': pd.date_range(start=year+'/06/01', end=year+'/08/31'),\n",
    "            'Fall': pd.date_range(start=year+'/09/01', end=year+'/11/30')}\n",
    "    if date_in in seasons['Spring']:\n",
    "        return 'Spring'\n",
    "    if date_in in seasons['Summer']:\n",
    "        return 'Summer'\n",
    "    if date_in in seasons['Fall']:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "    \n",
    "# Define Nearest Node Function\n",
    "def Closest_Nodes(gdA, gdB):\n",
    "\n",
    "    nA = np.array(list(gdA.geometry.apply(lambda x: (x.x, x.y))))\n",
    "    nB = np.array(list(gdB.geometry.apply(lambda x: (x.x, x.y))))\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "    gdB_nearest = gdB.iloc[idx].drop(columns=\"geometry\").reset_index(drop=True)\n",
    "    gdf = pd.concat(\n",
    "        [\n",
    "            gdA.reset_index(drop=True),\n",
    "            gdB_nearest,\n",
    "            pd.Series(dist, name='dist')\n",
    "        ], \n",
    "        axis=1)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop thru the dams to pull in RWC-Temp csvs, combine, etc. \n",
    "# Dam list needs to be int for file names\n",
    "Dam_List_File= list(map(int, Dams_List_Group))\n",
    "\n",
    "for i in Dam_List_File:\n",
    "    # Filter SWORD Nodes for the Dam \n",
    "    SWORD_Nodes_Dam = Updated_Profiles_UPDS[(Updated_Profiles_UPDS['Assgn_dam'] == i)]\n",
    "\n",
    "    # Get a list of the CSV files for the dam\n",
    "    CSVFiles = glob.glob(os.path.join(TempsFilePath, \"*_\"+ str(i)+\".csv\"))\n",
    "\n",
    "    # Loop through the files for each dam and make one dataframe\n",
    "    Combined_Dam = pd.DataFrame()\n",
    "    for j in range(len(CSVFiles)):\n",
    "        try:\n",
    "            x = pd.read_csv(CSVFiles[j])\n",
    "            Combined_Dam = pd.concat([Combined_Dam,x],axis=0)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(CSVFiles[j], \" is empty and has been skipped.\") # Some of the images create blank csvs -- could be ice/clouds/mask issues\n",
    "    \n",
    "    ## If this is empty skip to next dam\n",
    "    if Combined_Dam.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    #### Prep the RWC-TEMP Data ###\n",
    "    # Fix Date Time  -- GEE uses Unix  Epoch time\n",
    "    Combined_Dam['Date_Time'] = pd.to_datetime((Combined_Dam['GEE_time']/1000), unit = 's', utc=True)\n",
    "    Combined_Dam['Date'] = Combined_Dam['Date_Time'].map(pd.Timestamp.date)\n",
    "    Combined_Dam['Month'] = pd.DatetimeIndex(Combined_Dam['Date']).month\n",
    "    Combined_Dam['Day'] = pd.DatetimeIndex(Combined_Dam['Date']).day  \n",
    "    Combined_Dam['Year'] =  pd.DatetimeIndex(Combined_Dam['Date']).year\n",
    "\n",
    "    ## Add in Season\n",
    "    Combined_Dam['Season'] = Combined_Dam.Date.map(get_season)\n",
    "\n",
    "    ## Rename Width Column -- Clarity for Later\n",
    "    Combined_Dam.rename(columns={\"width\":\"RWC_wid\"}, inplace = True)\n",
    "\n",
    "    ## Filter out Ice\n",
    "    Combined_Dam_noice = Combined_Dam[Combined_Dam['GEE_temp']>0]\n",
    "\n",
    "    ## Make it a GFD\n",
    "    geometry = [Point(xy) for xy in zip(Combined_Dam_noice['longitude'], Combined_Dam_noice['latitude'])]\n",
    "    Combined_Dam_gdf = gpd.GeoDataFrame(Combined_Dam_noice, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    ### Get Each RWC Node's Nearest SWORD node  ####\n",
    "    Nearest_Nodes = Closest_Nodes(Combined_Dam_gdf, SWORD_Nodes_Dam)\n",
    "\n",
    "    # Nearest Distance is calculated in Degrees b/c both gdf are in WGS 84 --> (Convert Degrees to meters * 111139)\n",
    "    Nearest_Nodes['NDist_m'] = Nearest_Nodes['dist']*111139\n",
    "\n",
    "    ## Filtering to the closest nodes for averaging temps ##\n",
    "    # Get each SWORD node's 5 nearest RWC points for each date\n",
    "    Nearest_Nodes_grp = Nearest_Nodes.groupby(['Join_Node','Date'])['dist'].nsmallest(5)\n",
    "    Nearest_Nodes_grp = Nearest_Nodes_grp.reset_index()\n",
    "\n",
    "    # Get a list of Index numbers\n",
    "    Node_list5 = Nearest_Nodes_grp['level_2'].tolist()\n",
    "\n",
    "    # Select nodes by Index\n",
    "    Select_Nodes = Nearest_Nodes.iloc[Node_list5]\n",
    "\n",
    "    # Remove extra nodes -- If it is farther than 200m drop it  (distance to next SWORD node)\n",
    "    Nearest_Nodes_filt = Select_Nodes[Select_Nodes['NDist_m'] <= 200]\n",
    "\n",
    "    #### Get the Temperature Averages ####\n",
    "    ## Group By Node, Month, Day, and Year -- Get Average Temp \n",
    "    Date_Near_Temps = Nearest_Nodes_filt.groupby(['Join_Node','Month','Day','Year']).agg({'GEE_temp': ['mean'],'RWC_wid': ['mean']})\n",
    "    Date_Near_Temps.columns = ['Avg_Temp', 'Avg_RWC_Wid']\n",
    "    Date_Near_Temps =Date_Near_Temps.reset_index()\n",
    "\n",
    "    # Convert to Dataframe\n",
    "    AvgDateTemps_df = pd.DataFrame(Date_Near_Temps)\n",
    "    \n",
    "    # Join The information \n",
    "    AvgDateTemps_xy = pd.merge(AvgDateTemps_df, SWORD_Nodes_Dam, on='Join_Node', how='inner')\n",
    "    AvgDateTemps_xy = AvgDateTemps_xy[['Join_Node','Month','Day','Year', 'Avg_Temp','Avg_RWC_Wid', 'x','y','reach_id','lakeflag','Assgn_dam','Dam_Flag','Up_Ds', 'Dam_Dist', 'Dam_Dist_km']]\n",
    "\n",
    "    #### Export the Average Temps CSV ####\n",
    "    Export_File_Path = r\"F:\\Insert_File_Path_Here\" # Update the Filepath here ## These outputs will be used for analysis in Dammed_River_Temperatures_Analysis.ipynb\n",
    "    Dam_Name = i\n",
    "    AvgDateTemps_xy.to_csv(Export_File_Path+\"\\Dam_\"+ str(Dam_Name)+\"Avg_Img_Temps.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
